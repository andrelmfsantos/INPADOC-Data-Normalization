# -*- coding: utf-8 -*-
"""INPADOC_Data_Normalization_01_1_From_Web_of_Science_TXT_by_Year_to_CSV_Download_GoogleColab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tQdcBYhojbRBc7plwUgfIHENbZNAq4Y9

<pre>
<img align="center" width="900" src="https://raw.githubusercontent.com/andrelmfsantos/INPADOC-Data-Normalization/main/Images/FAPESP_Header_Google_Colab_english.png">
</pre>

> * __INPADOC: International Patent Documentation__
* [FAPESP Process Number: 23/12389-1](https://bv.fapesp.br/pt/auxilios/113767/solucoes-diagnosticas-e-terapeuticas-da-covid-19-protegidas-por-patentes-sistematizacao-das-principa/)

|   |   |
|--:|:--|
|**Authors:**|[Priscila Rezende da Costa](https://bv.fapesp.br/pt/pesquisador/67192/priscila-rezende-da-costa/) $-$ [Camila Naves Arantes](http://lattes.cnpq.br/3897204543440920) $-$ [Alex Fabianne de Paulo](http://lattes.cnpq.br/9690861410844635) $-$ </br> [Geciane Silveira Porto](https://bv.fapesp.br/pt/pesquisador/89388/geciane-silveira-porto/) $-$ [André Luis Marques Ferreira dos Santos](http://lattes.cnpq.br/9690861410844635) $-$ [Celise Marson](http://lattes.cnpq.br/2618279063609476)|
|**Host Institution:**|[Universidade Nove de Julho (UNINOVE). Campus Vergueiro. São Paulo , SP, Brasil](https://bv.fapesp.br/pt/instituicao/1496/campus-vergueiro/)|
|**Date:**|July 28, 2024|

**24,986 results from Derwent Innovations Index for:**

* Query:

    * TS=("covid" OR "coronavirus" OR "pandemic" OR "sars-cov-2")
* [Derwent $-$ Web of Science](https://www.webofscience.com/wos/diidw/summary/e80a62ba-ad88-42d0-a179-f69a58eb4087-fe8523ab/diidw-relevance/1)
* Export: "Tab delimited file"
* Record Content: Full Record

**Github files**:

* Exports_Web_of_Science_FY2019 :: savedrecs[0]
* Exports_Web_of_Science_FY2020 :: savedrecs[1 - 3]
* Exports_Web_of_Science_FY2021 :: savedrecs[4 - 10]
* Exports_Web_of_Science_FY2022 :: savedrecs[11 - 18]
* Exports_Web_of_Science_FY2023 :: savedrecs[19 - 24]
* Exports_Web_of_Science_FY2024 :: savedrecs[25 - 26]

**About this Notebook:**

1. **Library Imports:**
   - `pandas`: For data manipulation.
   - `os`: For file system operations.
   - `requests`: For making HTTP requests.
   - `zipfile`: For creating zip files.

2. **Directory Creation:**
   - Checks if the directory `TXT_Web_of_Science_to_CSV` exists and creates it if it doesn't.

3. **Base URL Configuration:**
   - Sets the base URL for accessing the files hosted on GitHub.

4. **Column Definition:**
   - List of column names to be used when creating DataFrames.

5. **Dictionary of Folders and Files:**
   - Maps folder names to lists of corresponding text files.

6. **Function to Read Files and Create DataFrame:**
   - Downloads the file from the URL.
   - Removes the BOM character from the first line if present.
   - Splits lines by tab characters and creates a list of dictionaries.
   - Constructs a DataFrame from the extracted data.

7. **Iteration over Folders and Files:**
   - For each folder and file listed in the dictionary:
     - Constructs the complete file URL.
     - Attempts to read the file and create a DataFrame.
     - Prints the number of rows in the DataFrame.
     - Saves the DataFrame to a CSV file in the output directory.
     - Prints a confirmation message of the save or an error message if the download fails.

8. **Creating a Zip File:**
   - Creates a zip file containing all the generated CSV files.

9. **Downloading the Zip File:**
   - Uses Google Colab to provide a download link for the zip file.
"""

import pandas as pd
import os
import requests
from zipfile import ZipFile

# Ensure the output directory exists
output_folder_path = 'TXT_Web_of_Science_to_CSV'
os.makedirs(output_folder_path, exist_ok=True)

# Base URL for the files on GitHub
base_url = 'https://raw.githubusercontent.com/andrelmfsantos/INPADOC-Data-Normalization/main/Exports_Web_of_Science_Full_Years/'

# Define column names
columns = ['PN', 'TI', 'AU', 'AE', 'GA', 'AB', 'TF', 'EA', 'DC', 'MC', 'IP', 'PD', 'AD', 'FD', 'PI', 'DS', 'FS', 'CP', 'CR', 'DN', 'MN', 'RI', 'CI', 'RG']

# Dictionary with folder names and respective file ranges
folders_files = {
    "Exports_Web_of_Science_FY2019": ["savedrecs_0.txt"],
    "Exports_Web_of_Science_FY2020": [f"savedrecs_{i}.txt" for i in range(1, 4)],
    "Exports_Web_of_Science_FY2021": [f"savedrecs_{i}.txt" for i in range(4, 11)],
    "Exports_Web_of_Science_FY2022": [f"savedrecs_{i}.txt" for i in range(11, 19)],
    "Exports_Web_of_Science_FY2023": [f"savedrecs_{i}.txt" for i in range(19, 25)],
    "Exports_Web_of_Science_FY2024": [f"savedrecs_{i}.txt" for i in range(25, 27)]
}

# Function to read a file from a URL and return a DataFrame
def read_file_to_dataframe(file_url):
    response = requests.get(file_url)
    response.raise_for_status()  # Ensure we notice bad responses
    lines = response.text.splitlines()
    if lines:
        lines[0] = lines[0].replace('\ufeff', '')  # Strip BOM character if present

        # Split lines by tab and create a list of dictionaries
        data = []
        for line in lines[1:]:  # Skip header line
            split_line = line.split('\t')
            entry = {col: split_line[i] if i < len(split_line) else None for i, col in enumerate(columns)}
            data.append(entry)

        # Create DataFrame
        df = pd.DataFrame(data, columns=columns)
        return df
    else:
        return pd.DataFrame(columns=columns)

# Iterate over each folder and file
for folder, files in folders_files.items():
    for file_name in files:
        file_url = f"{base_url}{folder}/{file_name}"
        try:
            df = read_file_to_dataframe(file_url)
            print(f'{file_name}: {len(df)} rows')

            # Save DataFrame to CSV
            #output_file_path = os.path.join(output_folder_path, f'{folder}_{file_name.replace(".txt", ".csv")}') # join folder name + file name to export
            output_file_path = os.path.join(output_folder_path, f'{file_name.replace(".txt", ".csv")}') # only file name to export
            df.to_csv(output_file_path, index=False)
            print(f'Saved {output_file_path}')
        except requests.exceptions.RequestException as e:
            print(f'Failed to download {file_url}: {e}')

# Zip the folder containing CSV files
zip_file = 'TXT_Web_of_Science_to_CSV.zip'
with ZipFile(zip_file, 'w') as zipf:
    for root, _, files in os.walk(output_folder_path):
        for file in files:
            zipf.write(os.path.join(root, file), arcname=file)

print(f"Created zip file: {zip_file}")

# Provide a link to download the zipped folder
from google.colab import files
files.download(zip_file)